---
title: 'Intoducing ClustImpute: A new approach for k-means clustering with build-in
  missing data imputation'
author: Oliver Pfaffel
date: '2019-06-20'
categories:
  - R
  - statistical analysis
  - unsupervised machine learning
tags:
  - clustering
  - imputation
  - missing data
  - k-means
slug: introducing-clustimpute
comments: yes
---



<p>We are happily introducing a new k-means clustering algorithm that includes a powerful multiple missing data imputation at the computational cost of a few extra random imputations (benchmarks following in a separate article). More precisely, the algorithm draws the missing values based on the current cluster assignment so that correlations are considered on this level (we assume a more granular dependence structure is not relevant if we are “only” interest in k partitions). Subsequently, penalizing weights are imposed on imputed values and succesively decreased (to zero) as the missing data imputation gets better. The hope is that at some point the observed point is near a cluster that provides a suitable neighborhood to draw the missing variable from. The algorithm is computationally efficient since the imputation is only as accurate as the clustering, and will be much faster than any approach that derives the full conditional missing distribution, e.g., as implemented in the (awesome) MICE package, independently of the clustering.</p>
<p>ClustImpute can currently be installed via github only:</p>
<pre class="r"><code># devtools::install_github(&quot;o1iv3r/ClustImpute&quot;)
library(ClustImpute)</code></pre>
<p>We’ll provide an example based on simulated data to emphasis the benefits of ClustImpute.</p>
<div id="simulated-data-with-missings" class="section level1">
<h1>Simulated data with missings</h1>
<p>First we create a random dataset with some structure and a few uncorrelated variables</p>
<pre class="r"><code>### Random Dataset
set.seed(739)
n &lt;- 7500 # numer of points
nr_other_vars &lt;- 4
mat &lt;- matrix(rnorm(nr_other_vars*n),n,nr_other_vars)
me&lt;-4 # mean
x &lt;- c(rnorm(n/3,me/2,1),rnorm(2*n/3,-me/2,1)) 
y &lt;- c(rnorm(n/3,0,1),rnorm(n/3,me,1),rnorm(n/3,-me,1))
true_clust &lt;- c(rep(1,n/3),rep(2,n/3),rep(3,n/3)) # true clusters
dat &lt;- cbind(mat,x,y)
dat&lt;- as.data.frame(scale(dat)) # scaling
summary(dat)
#&gt;        V1                 V2                  V3          
#&gt;  Min.   :-3.40352   Min.   :-4.273673   Min.   :-3.82710  
#&gt;  1st Qu.:-0.67607   1st Qu.:-0.670061   1st Qu.:-0.66962  
#&gt;  Median : 0.01295   Median :-0.006559   Median :-0.01179  
#&gt;  Mean   : 0.00000   Mean   : 0.000000   Mean   : 0.00000  
#&gt;  3rd Qu.: 0.67798   3rd Qu.: 0.684672   3rd Qu.: 0.67221  
#&gt;  Max.   : 3.35535   Max.   : 3.423416   Max.   : 3.80557  
#&gt;        V4                  x                 y            
#&gt;  Min.   :-3.652267   Min.   :-2.1994   Min.   :-2.151001  
#&gt;  1st Qu.:-0.684359   1st Qu.:-0.7738   1st Qu.:-0.975136  
#&gt;  Median : 0.001737   Median :-0.2901   Median : 0.009932  
#&gt;  Mean   : 0.000000   Mean   : 0.0000   Mean   : 0.000000  
#&gt;  3rd Qu.: 0.687404   3rd Qu.: 0.9420   3rd Qu.: 0.975788  
#&gt;  Max.   : 3.621530   Max.   : 2.8954   Max.   : 2.265420</code></pre>
<p>One can clearly see the three clusters</p>
<pre class="r"><code>plot(dat$x,dat$y)</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>We create 20% of missings using the custom function <strong>miss_sim()</strong></p>
<pre class="r"><code>dat_with_miss &lt;- miss_sim(dat,p=.2,seed_nr=120)
summary(dat_with_miss)
#&gt;        V1                V2                V3                V4         
#&gt;  Min.   :-3.4035   Min.   :-4.2737   Min.   :-3.8271   Min.   :-3.5844  
#&gt;  1st Qu.:-0.6756   1st Qu.:-0.6757   1st Qu.:-0.6634   1st Qu.:-0.6742  
#&gt;  Median : 0.0163   Median :-0.0104   Median :-0.0092   Median : 0.0194  
#&gt;  Mean   : 0.0024   Mean   :-0.0063   Mean   : 0.0027   Mean   : 0.0117  
#&gt;  3rd Qu.: 0.6886   3rd Qu.: 0.6683   3rd Qu.: 0.6774   3rd Qu.: 0.7010  
#&gt;  Max.   : 3.2431   Max.   : 3.4234   Max.   : 3.8056   Max.   : 3.6215  
#&gt;  NA&#39;s   :1513      NA&#39;s   :1499      NA&#39;s   :1470      NA&#39;s   :1486     
#&gt;        x                 y          
#&gt;  Min.   :-2.1994   Min.   :-2.1510  
#&gt;  1st Qu.:-0.7636   1st Qu.:-0.9745  
#&gt;  Median :-0.2955   Median : 0.0065  
#&gt;  Mean   : 0.0022   Mean   :-0.0019  
#&gt;  3rd Qu.: 0.9473   3rd Qu.: 0.9689  
#&gt;  Max.   : 2.8954   Max.   : 2.2654  
#&gt;  NA&#39;s   :1580      NA&#39;s   :1516
mis_ind &lt;- is.na(dat_with_miss) # missing indicator</code></pre>
<p>The correlation matrix of the missing indicator shows that the missings are correlated - thus we are not in a missing completely at random (MCAR) stetting:</p>
<pre class="r"><code>corrplot(cor(mis_ind),method=&quot;number&quot;)</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="typical-approach-median-or-random-imputation" class="section level1">
<h1>Typical approach: median or random imputation</h1>
<p>Clearly, an imputation with the median value does a pretty bad job here:</p>
<pre class="r"><code>dat_median_imp &lt;- dat_with_miss
for (j in 1:dim(dat)[2]) {
  dat_median_imp[,j] &lt;- Hmisc::impute(dat_median_imp[,j],fun=median)
}
imp &lt;- factor(pmax(mis_ind[,5],mis_ind[,6]),labels=c(&quot;Original&quot;,&quot;Imputed&quot;)) # point is imputed if x or y is imputed
ggplot(dat_median_imp) + geom_point(aes(x=x,y=y,color=imp))
#&gt; Don&#39;t know how to automatically pick scale for object of type impute. Defaulting to continuous.
#&gt; Don&#39;t know how to automatically pick scale for object of type impute. Defaulting to continuous.</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>But also a random imputation is not much better: it creates plenty of points in areas with no data</p>
<pre class="r"><code>dat_random_imp &lt;- dat_with_miss
for (j in 1:dim(dat)[2]) {
  dat_random_imp[,j] &lt;- impute(dat_random_imp[,j],fun=&quot;random&quot;)
}
imp &lt;- factor(pmax(mis_ind[,5],mis_ind[,6]),labels=c(&quot;Original&quot;,&quot;Imputed&quot;)) # point is imputed if x or y is imputed
ggplot(dat_random_imp) + geom_point(aes(x=x,y=y,color=imp))
#&gt; Don&#39;t know how to automatically pick scale for object of type impute. Defaulting to continuous.
#&gt; Don&#39;t know how to automatically pick scale for object of type impute. Defaulting to continuous.</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>A clustering based on random imputation will thus not provide good results (even if we “know” the number of clusters as in this example)</p>
<pre class="r"><code>tic(&quot;Clustering based on random imputation&quot;)
cl_compare &lt;- KMeans_arma(data=dat_random_imp,clusters=3,n_iter=100,seed=751)
toc()
#&gt; Clustering based on random imputation: 0.06 sec elapsed
dat_random_imp$pred &lt;- predict_KMeans(dat_random_imp,cl_compare)
ggplot(dat_random_imp) + geom_point(aes(x=x,y=y,color=factor(pred)))
#&gt; Don&#39;t know how to automatically pick scale for object of type impute. Defaulting to continuous.
#&gt; Don&#39;t know how to automatically pick scale for object of type impute. Defaulting to continuous.</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="better-approach-clustimpute" class="section level1">
<h1>Better approach: ClustImpute</h1>
<p>We’ll now use ClustImpute and also measure the run-time. In short, the algorithm follows these steps</p>
<ol style="list-style-type: decimal">
<li>It replaces all NAs by random imputation, i.e., for each variable with missings, it draws from the marginal distribution of this variable not taking into account any correlations with other variables</li>
<li>Weights &lt;1 are used to adjust the scale of an observation that was generated in step 1. The weights are calculated by a (linear) weight function that starts near zero and converges to 1 at n_end.</li>
<li>A k-means clustering is performed with a number of c_steps steps starting with a random initialization.</li>
<li>The values from step 2 are replaced by new draws conditionally on the assign cluster from step 3.</li>
<li>Steps 2-4 are repeated nr_iter times in total. The k-means clustering in step 3 uses the previous cluster centroids for initialization.</li>
<li>After the last draws a final k-means clustering is performed.</li>
</ol>
<p>The intuition is that points should be clustered with other points mainly based on their observed values, while the resulting clusters provide donors for the missing value imputation, so that step by step all variables can be used for the clustering.</p>
<pre class="r"><code>nr_iter &lt;- 10 # iterations of procedure
n_end &lt;- 10 # step until convergence of weight function to 1
nr_cluster &lt;- 3 # number of clusters
c_steps &lt;- 50 # numer of cluster steps per iteration
tic(&quot;Run ClustImpute&quot;)
res &lt;- ClustImpute(dat_with_miss,nr_cluster=nr_cluster, nr_iter=nr_iter, c_steps=c_steps, n_end=n_end) 
toc()
#&gt; Run ClustImpute: 0.64 sec elapsed</code></pre>
<p>ClustImpute provides several results:</p>
<pre class="r"><code>str(res)
#&gt; List of 5
#&gt;  $ complete_data  :&#39;data.frame&#39;: 7500 obs. of  6 variables:
#&gt;   ..$ V1: num [1:7500] 1.403 -0.309 -0.214 -1.286 -0.202 ...
#&gt;   ..$ V2: num [1:7500] -1.4579 -0.7899 -0.9775 -0.1607 -0.0413 ...
#&gt;   ..$ V3: num [1:7500] 0.7836 -0.3234 -2.149 -0.0461 0.3609 ...
#&gt;   ..$ V4: num [1:7500] 0.604 -0.427 -0.122 -1.287 -0.155 ...
#&gt;   ..$ x : num [1:7500] 2.57 1.2 1.48 1.15 1.65 ...
#&gt;   ..$ y : num [1:7500] -0.5077 -0.2453 0.022 0.0522 -0.0454 ...
#&gt;  $ clusters       : int [1:7500] 2 2 2 2 2 2 2 2 2 2 ...
#&gt;  $ centroids      : num [1:3, 1:6] 0.10554 -0.02642 -0.06682 0.14785 -0.00818 ...
#&gt;  $ imp_values_mean: num [1:11, 1:7] -0.0305 0.0214 -0.005 0.0558 -0.0297 ...
#&gt;   ..- attr(*, &quot;dimnames&quot;)=List of 2
#&gt;   .. ..$ : chr [1:11] &quot;mean_imp&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
#&gt;   .. ..$ : chr [1:7] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; ...
#&gt;  $ imp_values_sd  : num [1:11, 1:7] 0.992 0.954 0.978 0.997 0.983 ...
#&gt;   ..- attr(*, &quot;dimnames&quot;)=List of 2
#&gt;   .. ..$ : chr [1:11] &quot;sd_imp&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...
#&gt;   .. ..$ : chr [1:7] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; ...
#&gt;  - attr(*, &quot;class&quot;)= chr &quot;kmeans_ClustImpute&quot;
#&gt;  - attr(*, &quot;nr_iter&quot;)= num 10
#&gt;  - attr(*, &quot;c_steps&quot;)= num 50
#&gt;  - attr(*, &quot;wf&quot;)=function (n, n_end = 10)  
#&gt;   ..- attr(*, &quot;srcref&quot;)= &#39;srcref&#39; int [1:8] 153 15 156 1 15 1 155 158
#&gt;   .. ..- attr(*, &quot;srcfile&quot;)=Classes &#39;srcfilealias&#39;, &#39;srcfile&#39; &lt;environment: 0x0000000026b6ea78&gt; 
#&gt;  - attr(*, &quot;n_end&quot;)= num 10
#&gt;  - attr(*, &quot;seed_nr&quot;)= num 150519</code></pre>
<p>We’ll first look at the complete data and clustering results. Quite obviously, it gives better results then median / random imputation.</p>
<pre class="r"><code>ggplot(res$complete_data,aes(x,y,color=factor(res$clusters))) + geom_point()</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Packages like MICE compute a traceplot of mean and variance of the imputed variables for various chains. This diagnostics helps to show if the Markov chains converge to a stationary distribution.</p>
<p>Here we only have a single realization and thus re-run ClustImpute with various seeds to obtain different realizations.</p>
<pre class="r"><code>res2 &lt;- ClustImpute(dat_with_miss,nr_cluster=nr_cluster, nr_iter=nr_iter, c_steps=c_steps, n_end=n_end,seed_nr = 2)
res3 &lt;- ClustImpute(dat_with_miss,nr_cluster=nr_cluster, nr_iter=nr_iter, c_steps=c_steps, n_end=n_end,seed_nr = 3)
mean_all &lt;- rbind(res$imp_values_mean,res2$imp_values_mean,res3$imp_values_mean)
sd_all &lt;- rbind(res$imp_values_sd,res2$imp_values_sd,res3$imp_values_sd)

mean_all &lt;- cbind(mean_all,seed=rep(c(150519,2,3),each=11))
sd_all &lt;- cbind(sd_all,seed=rep(c(150519,2,3),each=11))</code></pre>
<p>The realizations mix nicely with each other, as the following plots shows. Thus it seems we obtain to a similar missing value distribution independently of the seed.</p>
<pre class="r"><code>ggplot(as.data.frame(mean_all)) + geom_line(aes(x=iter,y=V1,color=factor(seed))) + ggtitle(&quot;Mean&quot;)</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>ggplot(as.data.frame(sd_all)) + geom_line(aes(x=iter,y=V1,color=factor(seed))) + ggtitle(&quot;Std. dev.&quot;)</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-14-2.png" width="672" /></p>
</div>
<div id="quality-of-imputation-and-cluster-results" class="section level1">
<h1>Quality of imputation and cluster results</h1>
<div id="marginal-distributions" class="section level2">
<h2>Marginal distributions</h2>
<p>Now we compare the marginal distributions using a violin plot of x and y. In particular for y, the distribution by cluster is quite far away from the original distribution for the random imputation based clustering, but quite close for ClustImpute.</p>
<pre class="r"><code>dat4plot &lt;- dat
dat4plot$true_clust &lt;- true_clust
Xfinal &lt;- res$complete_data
Xfinal$pred &lt;- res$clusters

par(mfrow=c(1,2))
violinBy(dat4plot,&quot;x&quot;,&quot;true_clust&quot;,main=&quot;Original data&quot;)
violinBy(dat4plot,&quot;y&quot;,&quot;true_clust&quot;,main=&quot;Original data&quot;)</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>violinBy(Xfinal,&quot;x&quot;,&quot;pred&quot;,main=&quot;imputed data&quot;)
violinBy(Xfinal,&quot;y&quot;,&quot;pred&quot;,main=&quot;imputed data&quot;)</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
<pre class="r"><code>violinBy(dat_random_imp,&quot;x&quot;,&quot;pred&quot;,main=&quot;random imputation&quot;)
violinBy(dat_random_imp,&quot;y&quot;,&quot;pred&quot;,main=&quot;random imputation&quot;)</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-15-3.png" width="672" /></p>
</div>
<div id="external-validation-rand-index" class="section level2">
<h2>External validation: rand index</h2>
<p>Below we compare the rand index between true and fitted cluster assignment. For ClustImpute we obtain:</p>
<pre class="r"><code>external_validation(true_clust, res$clusters)
#&gt; [1] 0.6353923</code></pre>
<p>This is a much higher value than for a clsutering based on random imputation:</p>
<pre class="r"><code>class(dat_random_imp$pred) &lt;- &quot;numeric&quot;
external_validation(true_clust, dat_random_imp$pred)
#&gt; [1] 0.4284653</code></pre>
<p>Not surprisingly, the RandIndex for ClustImpute is much higher if we consider complete cases only (and throw away a considerable amount of our data).</p>
<pre class="r"><code>## complete cases
idx &lt;- which(complete.cases(dat_with_miss)==TRUE)
sprintf(&quot;Number of complete cases is %s&quot;,length(idx))
#&gt; [1] &quot;Number of complete cases is 2181&quot;
sprintf(&quot;Rand index for this case %s&quot;, external_validation(true_clust[idx], res$clusters[idx]))
#&gt; [1] &quot;Rand index for this case 0.975286775813841&quot;</code></pre>
<p>Aside from the RandIndex, this function also computes a variety of other stats</p>
<pre class="r"><code>external_validation(true_clust, res$clusters,summary_stats = TRUE)
#&gt;  
#&gt; ---------------------------------------- 
#&gt; purity                         : 0.8647 
#&gt; entropy                        : 0.4397 
#&gt; normalized mutual information  : 0.5595 
#&gt; variation of information       : 1.3956 
#&gt; normalized var. of information : 0.6116 
#&gt; ---------------------------------------- 
#&gt; specificity                    : 0.8778 
#&gt; sensitivity                    : 0.7579 
#&gt; precision                      : 0.7561 
#&gt; recall                         : 0.7579 
#&gt; F-measure                      : 0.757 
#&gt; ---------------------------------------- 
#&gt; accuracy OR rand-index         : 0.8379 
#&gt; adjusted-rand-index            : 0.6354 
#&gt; jaccard-index                  : 0.6091 
#&gt; fowlkes-mallows-index          : 0.757 
#&gt; mirkin-metric                  : 9118230 
#&gt; ----------------------------------------
#&gt; [1] 0.6353923</code></pre>
</div>
<div id="variance-reduction" class="section level2">
<h2>Variance reduction</h2>
<p>To assess quality of our cluster results, we compute the sum of squares within each cluster, sum up these values and compare it with the total sum of squares.</p>
<pre class="r"><code>res_var &lt;- var_reduction(res)
res_var$Variance_reduction
#&gt; [1] 0.2795489
res_var$Variance_by_cluster
#&gt; # A tibble: 1 x 6
#&gt;      V1    V2    V3    V4     x     y
#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
#&gt; 1 0.987 0.983  1.01 0.976 0.218 0.117</code></pre>
<p>We se a reduction of about 27% using only 3 clusters, most strikingly for x and y because that these variables define the subspace of the true clusters.</p>
<p>More clusters will capture the random distribution of the other variables</p>
<pre class="r"><code>res &lt;- ClustImpute(dat_with_miss,nr_cluster=10, nr_iter=nr_iter, c_steps=c_steps, n_end=n_end)
res_var &lt;- var_reduction(res)
res_var$Variance_reduction
#&gt; [1] 0.5209119
res_var$Variance_by_cluster
#&gt; # A tibble: 1 x 6
#&gt;      V1    V2    V3    V4     x     y
#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
#&gt; 1 0.568 0.601 0.609 0.616 0.267 0.187</code></pre>
<p>Can we use this function systematically to find out the optimal number of clusters? We’ll do the exercise above for a several values of nr_cluster and need a helper function for that since X is an argument of ClustImpute</p>
<pre class="r"><code>ClustImpute2 &lt;- function(dataFrame,nr_cluster, nr_iter=10, c_steps=1, wf=default_wf, n_end=10, seed_nr=150519) {
  return(ClustImpute(dataFrame,nr_cluster, nr_iter, c_steps, wf, n_end, seed_nr))
}
res_list &lt;- lapply(X=1:10,FUN=ClustImpute2,dataFrame=dat_with_miss, nr_iter=nr_iter, c_steps=c_steps, n_end=n_end)</code></pre>
<p>Nex we put the variances by cluster in a table</p>
<pre class="r"><code>tmp &lt;- var_reduction(res_list[[1]])
var_by_clust &lt;- tmp$Variance_by_cluster
for (k in 2:10) {
  tmp &lt;- var_reduction(res_list[[k]])
  var_by_clust &lt;- rbind(var_by_clust,tmp$Variance_by_cluster)
}
var_by_clust$nr_clusters &lt;- 1:10</code></pre>
<p>While there is a rather gradual improvement for the other variables, x and y have a minimum at 3 showing optimality for these variables. Such a plot clearly indicates that 3 clusters are a good choice for this data set (which, of course, we knew in advance)</p>
<pre class="r"><code>data2plot &lt;- tidyr::gather(var_by_clust,key = &quot;variable&quot;, value = &quot;variance&quot;, -dplyr::one_of(&quot;nr_clusters&quot;))
ggplot(data2plot,aes(x=nr_clusters,y=variance,color=variable)) + geom_line() + scale_x_continuous(breaks=1:10)</code></pre>
<p><img src="/post/2019-06-20-intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Now it’s time for you to try this algorithm on real problems! Looking forward to feedback either in the comment section or on the github page.</p>
</div>
</div>
