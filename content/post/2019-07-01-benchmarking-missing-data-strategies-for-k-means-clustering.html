---
title: Benchmarking missing data strategies for k-means clustering
author: Oliver Pfaffel
date: '2019-07-01'
slug: benchmarking-missing-data-strategies-for-k-means-clustering
categories:
  - R
  - statistical analysis
  - unsupervised machine learning
tags:
  - benchmarking
  - clustering
  - imputation
  - missing data
  - k-means
description: ''
thumbnail: ''
---



<p>The goal is to compare a few algorithms for missing imputation when used before k-means clustering is performed. For the latter we use the same algorithm as in ClustImpute to ensure that only the computation time of the imputation is compared. In a nutshell, we’ll se that ClustImpute scales like a random imputation and hence is much faster than a pre-processing with MICE / MissRanger. This is not surprising since ClustImpute basically runs a fixed number of random imputations conditional on the current cluster assignment. Below is the code that has been used.</p>
<p>First we’ll load all relevant packages</p>
<p>Then we define a function that creates us an artificial data set with variable size <span class="math inline">\(n\)</span>.</p>
<pre class="r"><code>create_random_data &lt;- function(n,nr_other_vars=4,seedvar=739) {
  n &lt;- round(n/3)*3
  set.seed(seedvar)
  mat &lt;- matrix(rnorm(nr_other_vars*n),n,nr_other_vars)
  me&lt;-4 # mean
  x &lt;- c(rnorm(n/3,me/2,1),rnorm(2*n/3,-me/2,1)) 
  y &lt;- c(rnorm(n/3,0,1),rnorm(n/3,me,1),rnorm(n/3,-me,1))
  true_clust &lt;- c(rep(1,n/3),rep(2,n/3),rep(3,n/3)) # true clusters
  dat &lt;- cbind(mat,x,y)
  dat&lt;- as.data.frame(scale(dat)) # scaling
  
  return(list(data=dat,true_clusters=true_clust))
}</code></pre>
<p>The parametrization <span class="math inline">\(nr_iter_other\)</span> of the clustering for external imputation strategies is set so that we have a comparable number of steps after imputation (steps of Clustimpute with a weight &lt; 1 are considered the burn-in phase since only afterwards the distribution is considered credible). Due to a very slow performance we decided to drop MissForrest and only consider MissRanger. The results below are for a missing rate of 20%, though this could be changed easily. For each data set, imputation and clustering are repeated 5 times with a differnt seed each time.</p>
<pre class="r"><code>### for various n and p
N &lt;- 2^(0:4)*400
p &lt;- .2</code></pre>
<pre class="r"><code>### Parameters
# ClustImpute
nr_iter &lt;- 14 # iterations of procedure
n_end &lt;- 10 # step until convergence of weight function to 1
nr_cluster &lt;- 3 # number of clusters
c_steps &lt;- 50 # numer of cluster steps per iteration
# Clustering based on imputation
nr_iter_other &lt;- (nr_iter-n_end) * c_steps # comparable number of steps &quot;after&quot; imputation

param_times &lt;- 5 # how often to compute the benchmark

results &lt;- list()
count &lt;- 0

for (n in N) {
  count &lt;- count + 1
  # Create random data with missings
  random_data &lt;- create_random_data(n)
  dat &lt;- random_data$data
  dat_with_miss &lt;- miss_sim(dat,p,seed_nr=123)
  
  ### Benchmark time and calculate rand index
  rand_ClustImpute &lt;- 0
  rand_missRanger &lt;- 0
  rand_RandomImp &lt;- 0
  rand_mice &lt;- 0
  rand_amelia &lt;- 0
  
  # Use a different seed each time
  mbm &lt;- microbenchmark(&quot;ClustImpute&quot; = {
    res &lt;- ClustImpute(dat_with_miss,nr_cluster=nr_cluster, nr_iter=nr_iter, c_steps=c_steps, n_end=n_end, seed_nr = random_seed)
    rand_ClustImpute &lt;- rand_ClustImpute + external_validation(random_data$true_clusters, res$clusters)
  }, &quot;RandomImp&quot;={
    dat_random_imp &lt;- dat_with_miss
    for (j in 1:dim(dat)[2]) {
      dat_random_imp[,j] &lt;- impute(dat_random_imp[,j],fun=&quot;random&quot;)
    }
    cl_RandomImp &lt;- KMeans_arma(data=dat_random_imp,clusters=3,n_iter=nr_iter_other,seed=random_seed)
    pred_RandomImp &lt;- predict_KMeans(dat_random_imp,cl_RandomImp)
    class(pred_RandomImp) &lt;- &quot;numeric&quot;
    rand_RandomImp &lt;- rand_RandomImp + external_validation(random_data$true_clusters, pred_RandomImp)
  },
  &quot;MissRanger&quot;={
    imp_missRanger &lt;- missRanger(dat_with_miss,pmm.k = 3)
    cl_missRanger &lt;- KMeans_arma(data=imp_missRanger,clusters=3,n_iter=nr_iter_other,seed=random_seed)
    pred_missRanger &lt;- predict_KMeans(imp_missRanger,cl_missRanger)
    class(pred_missRanger) &lt;- &quot;numeric&quot;
    rand_missRanger &lt;- rand_missRanger + external_validation(random_data$true_clusters, pred_missRanger)
  }, 
  &quot;MICE&quot;={
    dat_mice &lt;- mice(dat_with_miss,m=1,maxit=50,meth=&#39;pmm&#39;) # single data set
    dat_mice &lt;- complete(dat_mice)
    cl_mice &lt;- KMeans_arma(data=dat_mice,clusters=3,n_iter=nr_iter_other,seed=random_seed)
    pred_mice &lt;- predict_KMeans(dat_mice,cl_mice)
    class(pred_mice) &lt;- &quot;numeric&quot;
    rand_mice &lt;- rand_mice + external_validation(random_data$true_clusters, pred_mice)
  }, &quot;AMELIA&quot;= {
    dat_amelia &lt;- amelia(dat_with_miss,m=1) # single data set
    dat_amelia &lt;- dat_amelia$imputations$imp1
    cl_amelia &lt;- KMeans_arma(data=dat_amelia,clusters=3,n_iter=nr_iter_other,seed=random_seed)
    pred_amelia &lt;- predict_KMeans(dat_amelia,cl_amelia)
    class(pred_amelia) &lt;- &quot;numeric&quot;
    rand_amelia &lt;- rand_amelia + external_validation(random_data$true_clusters, pred_amelia)
    
  }  ,times = param_times, setup = {random_seed=round(runif(1)*1e5)}, unit = &quot;s&quot;)
  
  # compute average rand index
  rand_ClustImpute &lt;- rand_ClustImpute/param_times
  rand_missRanger &lt;- rand_missRanger/param_times
  rand_RandomImp &lt;- rand_RandomImp/param_times
  rand_mice &lt;- rand_mice/param_times
  rand_amelia &lt;- rand_amelia/param_times
  
  results$randIndex[[count]] &lt;- c(ClustImpute=rand_ClustImpute,RandomImp=rand_RandomImp,#missForest=rand_missForest,
                                  missRanger=rand_missRanger,MICE=rand_mice,AMELIA=rand_amelia)

  results$benchmark[[count]] &lt;- mbm
  
  mbm_median &lt;- print(mbm)$median
  results$benchmark_median[[count]] &lt;- mbm_median
}</code></pre>
<p>Below are the results for the four data sets (only differing in size). Clearly a reandom imputation is fastest, but the AMELIA package is only slightly slower. ClustImpute is 3rd and considerably faster than MICE and missRanger.</p>
<pre class="r"><code>plot_grid(autoplot(results$benchmark[[2]]),autoplot(results$benchmark[[3]]),autoplot(results$benchmark[[4]]),
          autoplot(results$benchmark[[5]]),
               ncol=2,labels=sprintf(&quot;n = %s&quot;,N),label_size = 12,vjust=1)</code></pre>
<p><img src="/post/2019-07-01-benchmarking-missing-data-strategies-for-k-means-clustering_files/figure-html/unnamed-chunk-6-1.png" width="960" /></p>
<p>Let’s focus on the scalabiltiy here: Amelia and ClustImpute scale much better than MICE and missRanger. ClustImpute scales like a simple random imputation and similarly as AMELIA.</p>
<pre class="r"><code>data2plot &lt;- data.frame(median=unlist(results$benchmark_median))
data2plot$method &lt;- rep(Hmisc::Cs(ClustImpute,RandomImp,MissRanger,MICE,AMELIA),length(N))
data2plot$n &lt;- rep(N,each=5)

# with shared legend
ps1 &lt;- ggplot(data2plot,aes(x=n,y=median,color=method)) + geom_point() + theme_cowplot() + geom_smooth() + 
  xlab(&quot;Numer of observations n&quot;) + ylab(&quot;Median running time in seconds&quot;)
legend &lt;- get_legend(ps1)
ps2 &lt;- ggplot(data2plot,aes(x=n,y=median,color=method)) + geom_point() + theme_cowplot() + geom_smooth() + 
  scale_y_log10() + xlab(&quot;Numer of observations n&quot;) + ylab(&quot;Median running time in seconds&quot;) + theme(legend.position=&quot;none&quot;)
plot_grid(ps1 + theme(legend.position=&quot;none&quot;),ps2,legend,nrow=1,rel_widths = c(3,3,1))</code></pre>
<p><img src="/post/2019-07-01-benchmarking-missing-data-strategies-for-k-means-clustering_files/figure-html/unnamed-chunk-7-1.png" width="960" /></p>
<p>Of course, above plots only consider the running time. Below is a table of the rand indices comparing the resulting clusters with the true clusters. ClustImpute provides the highest numbers, although the other imputation methods (except random imputation) could provide better results if they are tuned.</p>
<pre class="r"><code>randtbl &lt;- data.frame(matrix(unlist(results$randIndex),nrow=length(results$randIndex), byrow=T))
colnames(randtbl) &lt;- names(results$randIndex[[1]])
randtbl$N &lt;- N
knitr::kable(x=randtbl)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">ClustImpute</th>
<th align="right">RandomImp</th>
<th align="right">missRanger</th>
<th align="right">MICE</th>
<th align="right">AMELIA</th>
<th align="right">N</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.6784811</td>
<td align="right">0.3162996</td>
<td align="right">0.3031195</td>
<td align="right">0.2920014</td>
<td align="right">0.3529032</td>
<td align="right">400</td>
</tr>
<tr class="even">
<td align="right">0.6518900</td>
<td align="right">0.3149466</td>
<td align="right">0.3107541</td>
<td align="right">0.3186570</td>
<td align="right">0.2839927</td>
<td align="right">800</td>
</tr>
<tr class="odd">
<td align="right">0.6895598</td>
<td align="right">0.4122350</td>
<td align="right">0.2648335</td>
<td align="right">0.2978446</td>
<td align="right">0.2051036</td>
<td align="right">1600</td>
</tr>
<tr class="even">
<td align="right">0.6732008</td>
<td align="right">0.3458818</td>
<td align="right">0.3365066</td>
<td align="right">0.4451816</td>
<td align="right">0.4263087</td>
<td align="right">3200</td>
</tr>
<tr class="odd">
<td align="right">0.6672859</td>
<td align="right">0.2768001</td>
<td align="right">0.3134265</td>
<td align="right">0.2249913</td>
<td align="right">0.3039771</td>
<td align="right">6400</td>
</tr>
</tbody>
</table>
